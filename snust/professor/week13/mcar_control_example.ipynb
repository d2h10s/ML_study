{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposed-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym, random, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protected-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mcar_env:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.env._max_episode_steps = 500\n",
    "        self.pos_space = np.linspace(-1.2,0.6, 19)\n",
    "        self.vel_space = np.linspace(-0.07, 0.07, 19)\n",
    "        \n",
    "        self.actions = [0,1,2]\n",
    "        self.states = [ (i,j) for i in range(20) for j in range(20)]\n",
    "        \n",
    "    def get_agent_state(self, obs):\n",
    "        pos, vel = obs\n",
    "        pos_num = np.digitize(pos, self.pos_space)\n",
    "        vel_num = np.digitize(vel, self.vel_space)\n",
    "        agent_state = (pos_num, vel_num)\n",
    "        return agent_state\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return self.get_agent_state(obs)\n",
    "    \n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "    \n",
    "    def step(self, action):\n",
    "        sp, r, d, info = self.env.step(action)\n",
    "        return self.get_agent_state(sp), r, d, info\n",
    "    \n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "innocent-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_env = mcar_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "located-snowboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-500.0\n"
     ]
    }
   ],
   "source": [
    "d = False\n",
    "new_env.reset()\n",
    "total_r = 0\n",
    "while not d:\n",
    "    a = random.randint(0,2)\n",
    "    _,r,d,_ = new_env.step(a)\n",
    "    new_env.render()\n",
    "    total_r += r\n",
    "new_env.close()\n",
    "print(total_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operating-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent:\n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env     = env\n",
    "        self.actions = self.env.actions\n",
    "        self.states = self.env.states\n",
    "        self.Q = {}\n",
    "        for state in self.states:\n",
    "            for a in self.actions:\n",
    "                self.Q[(state, a)] = 0\n",
    "        \n",
    "    def action(self, obs, ep=0):\n",
    "        coin = np.random.rand()\n",
    "        if coin < ep:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            Qvalues =[self.Q[(obs, j)] for j in self.actions]\n",
    "            return np.argmax(Qvalues)\n",
    "    \n",
    "    def test(self, test_num, render_op = False):\n",
    "        \n",
    "        aver_r = 0\n",
    "        \n",
    "        for j in range(test_num):\n",
    "            total_r = 0\n",
    "            done = False\n",
    "            s    = self.env.reset()\n",
    "        \n",
    "            while not done:\n",
    "                if render_op:\n",
    "                    self.env.render()\n",
    "                a = self.action(s)\n",
    "                s, r, done, _ = self.env.step(a)\n",
    "                total_r += r\n",
    "                aver_r  += r\n",
    "            \n",
    "            if render_op:\n",
    "                self.env.render()\n",
    "                print(\"{}-th episode reward : {}\".format(j+1, total_r))\n",
    "            total_r = 0\n",
    "        \n",
    "        self.env.close()\n",
    "        return aver_r / test_num\n",
    "    \n",
    "    def saveQ(self, name):\n",
    "        f = open(name +\".pkl\", \"wb\")\n",
    "        pickle.dump(self.Q, f)\n",
    "        f.close()\n",
    "        print(\"Q-values are saved\")\n",
    "            \n",
    "    def loadQ(self, name):\n",
    "        with open( name +'.pkl', 'rb') as f:\n",
    "            self.Q = pickle.load(f)\n",
    "        print(\"Q-values aer loaded\")\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "further-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = Qagent(new_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "younger-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-500.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.test(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pending-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    def __init__(self,agent, lr, gamma, init_ep, min_ep, ep_decay):\n",
    "        self.agent = agent\n",
    "        self.lr    = lr\n",
    "        self.gamma = gamma\n",
    "        self.ep = init_ep\n",
    "        self.min_ep  = min_ep\n",
    "        self.ep_decay = ep_decay\n",
    "        \n",
    "    def learning(self, num_training):\n",
    "        \n",
    "        reward_history = []\n",
    "        test_history = []\n",
    "        \n",
    "        for j in range(num_training):\n",
    "            \n",
    "            done = False\n",
    "            total_r = 0\n",
    "            s = self.agent.env.reset()\n",
    "                \n",
    "            if self.ep > self.min_ep and (j+1) % self.ep_decay == 0:\n",
    "                self.ep -= 0.1\n",
    "            \n",
    "            \n",
    "            while not done:\n",
    "                a = self.agent.action(s, self.ep)\n",
    "                sp, r, done, _ = self.agent.env.step(a)\n",
    "                total_r += r\n",
    "                next_q_values = [self.agent.Q[(sp,jj)] for jj in self.agent.actions]\n",
    "                next_q_max = np.max(next_q_values)\n",
    "                self.agent.Q[(s,a)] +=self.lr *( r + (1-int(done))*self.gamma *next_q_max - self.agent.Q[(s,a)])\n",
    "                s = sp\n",
    "                \n",
    "            reward_history.append(total_r)\n",
    "            if (j+1)%self.ep_decay == 0:\n",
    "                avr = self.agent.test(10)\n",
    "                print(\"Trained episode so far :{}  average test reward {}\".format(j+1, avr))\n",
    "                test_history.append(avr)\n",
    "                \n",
    "            \n",
    "                \n",
    "        self.agent.env.close()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "copyrighted-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = Qagent(new_env)\n",
    "tt = trainer(agent=Agent, lr=0.05, gamma=0.99,\n",
    "             init_ep=1.0, min_ep=0.1, ep_decay=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efficient-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained episode so far :200  average test reward -500.0\n",
      "Trained episode so far :400  average test reward -500.0\n",
      "Trained episode so far :600  average test reward -136.3\n",
      "Trained episode so far :800  average test reward -176.8\n",
      "Trained episode so far :1000  average test reward -189.5\n",
      "Trained episode so far :1200  average test reward -152.9\n",
      "Trained episode so far :1400  average test reward -196.9\n",
      "Trained episode so far :1600  average test reward -187.0\n",
      "Trained episode so far :1800  average test reward -188.9\n",
      "Trained episode so far :2000  average test reward -279.0\n",
      "Trained episode so far :2200  average test reward -269.3\n",
      "Trained episode so far :2400  average test reward -172.7\n",
      "Trained episode so far :2600  average test reward -214.2\n"
     ]
    }
   ],
   "source": [
    "tt.learning(2600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bronze-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th episode reward : -209.0\n",
      "2-th episode reward : -214.0\n",
      "3-th episode reward : -209.0\n",
      "4-th episode reward : -248.0\n",
      "5-th episode reward : -220.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-220.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent.saveQ('mcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "underlying-subdivision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values aer loaded\n"
     ]
    }
   ],
   "source": [
    "Agent.loadQ('mcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bronze-satellite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th episode reward : -157.0\n",
      "2-th episode reward : -156.0\n",
      "3-th episode reward : -184.0\n",
      "4-th episode reward : -177.0\n",
      "5-th episode reward : -173.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-169.4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-peter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
