{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random, math, copy, gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(Qagent, self).__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.input_dim = self.env.observation_space.shape[0]\n",
    "        self.output_dim = self.env.action_space.n\n",
    "        self.fc1 = nn.Linear(self.input_dim, 240)\n",
    "        self.fc2 = nn.Linear(240, 240)\n",
    "        self.fc3 = nn.Linear(240, self.output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def action(self, state, epsilon=0):\n",
    "        qvalues = self.forward(torch.from_numpy(state).float())\n",
    "        coin = random.random()\n",
    "        # epsilon-greedy action 선택\n",
    "        if coin < epsilon:\n",
    "            a = random.randint(0,self.output_dim-1)\n",
    "        else : \n",
    "            a = qvalues.argmax().item() \n",
    "    \n",
    "        return a \n",
    "    \n",
    "    def test(self, test_num, render_op=False):\n",
    "        \n",
    "        reward_history =[]\n",
    "        \n",
    "        for i in range(test_num):\n",
    "            s = self.env.reset()\n",
    "            d = False\n",
    "            epi_r = 0\n",
    "            \n",
    "            while not d:\n",
    "                \n",
    "                if render_op:\n",
    "                    self.env.render()\n",
    "                a = self.action(s, epsilon=0) #greedy action     \n",
    "                s, r, d, info = self.env.step(a)\n",
    "                epi_r += r\n",
    "            \n",
    "            reward_history.append(epi_r)\n",
    "            if render_op:\n",
    "                print(\"{}-번째 테스크 결과는 {}\".format(i+1, epi_r))\n",
    "            \n",
    "        self.env.close()\n",
    "        if render_op:\n",
    "            print(\"{}-번 게임의 평균 점수는 {}, 최고점수는 {}\".format(test_num, np.mean(reward_history),np.max(reward_history)))\n",
    "        else:\n",
    "            return np.mean(reward_history)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def save_models(self, name=None):\n",
    "        if name == None:\n",
    "            name = 'noname'\n",
    "        torch.save(self.state_dict(), name + '_qnet.pt')\n",
    "        print('Models saved successfully')\n",
    "    \n",
    "    def load_models(self, name=None):\n",
    "        if name ==None:\n",
    "            name = 'noname'\n",
    "        self.load_state_dict(torch.load(name + '_qnet.pt'))\n",
    "        print ('Models loaded succesfully')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "class hp:   #collection of hyper parameters\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.save_file_name = 'mountain_car_dqn' #여러분이 원하는 이름 \n",
    "        \n",
    "        self.lr = 3e-4\n",
    "        self.gamma = 0.99\n",
    "        self.num_training = int(5e4) \n",
    "        self.buffersize = int(2e3)  \n",
    "        self.minibatchsize = 24 \n",
    "        self.start_from = int(1e2) \n",
    "        self.print_interval = int(2e3)\n",
    "        self.target_update_interval = int(1e3) \n",
    "        self.init_ep = 1\n",
    "        self.min_ep = 0.01\n",
    "        self.decay = int(1e3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, buffer_limit, mini_size):\n",
    "        \n",
    "        self.buffer = deque(maxlen=buffer_limit)\n",
    "        self.batch_size = mini_size\n",
    "    \n",
    "    \n",
    "    def put(self, sarsd):\n",
    "        self.buffer.append(sarsd)\n",
    "    \n",
    "    def sample(self):\n",
    "        mini_batch = random.sample(self.buffer, self.batch_size)\n",
    "        s_list, a_list, r_list, s_prime_list, done_list = [], [], [], [], []\n",
    "        \n",
    "        for sarsd in mini_batch:\n",
    "            s, a, r, s_prime, done = sarsd\n",
    "            s_list.append(s)\n",
    "            a_list.append([a])\n",
    "            r_list.append([r])\n",
    "            s_prime_list.append(s_prime)\n",
    "            done_list.append([done])\n",
    "\n",
    "        return torch.FloatTensor(s_list), torch.tensor(a_list), torch.FloatTensor(r_list),\\\n",
    "                torch.FloatTensor(s_prime_list), torch.FloatTensor(done_list)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, agent, env, hps):\n",
    "        \n",
    "        self.agent = agent\n",
    "        self.env   = env\n",
    "        self.targetA = copy.deepcopy(self.agent)\n",
    "        self.targetupdate()\n",
    "        self.hps     = hps\n",
    "        \n",
    "        self.lrate = self.hps.lr\n",
    "        self.gamma = self.hps.gamma\n",
    "        self.ram =  ReplayBuffer(self.hps.buffersize, self.hps.minibatchsize)\n",
    "        self.target_update_interval = self.hps.target_update_interval\n",
    "        \n",
    "        self.update_ctr = 0\n",
    "        self.training_ctr = 0\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=self.lrate)\n",
    "        \n",
    "    \n",
    "    def targetupdate(self):\n",
    "        return self.targetA.load_state_dict(self.agent.state_dict())\n",
    "    \n",
    "    def plot(self, frame_idx, rewards, test_score, losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(131)\n",
    "        plt.title('Frame %s k. last 10 epi reward mean: %.2f' % (int(frame_idx/1000), np.mean(rewards[-10:])))\n",
    "        plt.plot(rewards, 'k-')\n",
    "        plt.subplot(132)\n",
    "        plt.title('Test Score')\n",
    "        plt.plot(test_score, 'r--')\n",
    "        plt.subplot(133)\n",
    "        plt.title('Loss')\n",
    "        plt.plot(losses)\n",
    "        plt.savefig(str(self.hps.save_file_name)+'.png')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "            \n",
    "        s,a,r,s_prime,done = self.ram.sample()\n",
    "        qvalues = self.agent(s)\n",
    "        q_a = qvalues.gather(1,a)\n",
    "        with torch.no_grad():\n",
    "            max_q_sprime = self.targetA(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + self.gamma * max_q_sprime * done\n",
    "        loss = F.mse_loss(q_a, target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "            \n",
    "    def learning(self):\n",
    "        self.reward_history =[]\n",
    "        self.test_history = []\n",
    "        self.loss_history =[]\n",
    "        \n",
    "        ep = lambda frame_num : self.hps.min_ep + (self.hps.init_ep - self.hps.min_ep) * math.exp(-1. * frame_num / self.hps.decay) \n",
    "        \n",
    "        s = self.env.reset()\n",
    "        d = False\n",
    "        epi_r = 0\n",
    "            \n",
    "        \n",
    "        while self.training_ctr < self.hps.num_training:\n",
    "            epsilon = ep(self.training_ctr)\n",
    "            a = self.agent.action(s, epsilon)      \n",
    "            s_prime, r, d, info = self.env.step(a)\n",
    "            epi_r += r\n",
    "            done = 0.0 if d else 1.0\n",
    "            self.ram.put((s,a,r/50.0, s_prime, done))\n",
    "            if d:\n",
    "                s = self.env.reset()\n",
    "                d = False\n",
    "                self.reward_history.append(epi_r)\n",
    "                epi_r = 0\n",
    "            else:\n",
    "                s = s_prime\n",
    "            \n",
    "            \n",
    "            if self.ram.size()> self.hps.start_from:\n",
    "                self.update_ctr +=1\n",
    "                loss = self.train()\n",
    "                self.loss_history.append(loss)\n",
    "                \n",
    "            if (self.update_ctr +1) % self.target_update_interval ==0:\n",
    "                self.targetupdate()\n",
    "            \n",
    "            if (self.training_ctr+1)%self.hps.print_interval==0:\n",
    "                self.test_history.append(self.agent.test(20))\n",
    "                if np.max(self.test_history) == self.test_history[-1]:\n",
    "                    self.agent.save_models(self.hps.save_file_name)\n",
    "                self.plot(self.training_ctr+1, self.reward_history, self.test_history, self.loss_history)\n",
    "            self.training_ctr +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = hp()\n",
    "agent = Qagent(env)\n",
    "trainer = DQN(agent, env, hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_models('mountain_car_dqn') #여러분이 제출하는 저장 파일 입력\n",
    "agent.test(20, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-springfield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-priority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-recorder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-thing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
