{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thousand-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monetary-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    " # MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software. #\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit fashion mnist 데이터 셋의 일부를 약간 변형하여 만든 과제를 위한 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cathedral-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow keras dataset에서 가져온 데이타셋.\n",
    "train_image = np.load('hw_train_image.npy')\n",
    "train_label = np.load('hw_train_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boxed-humanity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(18000, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legislative-textbook",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 144x144 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"141.655915pt\" version=\"1.1\" viewBox=\"0 0 142.845 141.655915\" width=\"142.845pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-05-31T12:03:26.751920</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 141.655915 \nL 142.845 141.655915 \nL 142.845 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 117.77779 \nL 135.645 117.77779 \nL 135.645 9.05779 \nL 26.925 9.05779 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p330d78e773)\">\n    <image height=\"109\" id=\"image7eab3cfac5\" transform=\"scale(1 -1)translate(0 -109)\" width=\"109\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-8.77779\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m6fce3a0716\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.866429\" xlink:href=\"#m6fce3a0716\" y=\"117.77779\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(25.685179 132.376228)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"67.695\" xlink:href=\"#m6fce3a0716\" y=\"117.77779\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(61.3325 132.376228)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.523571\" xlink:href=\"#m6fce3a0716\" y=\"117.77779\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(100.161071 132.376228)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m01f25df62b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m01f25df62b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m01f25df62b\" y=\"49.82779\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 53.627009)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m01f25df62b\" y=\"88.656362\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 92.45558)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 117.77779 \nL 26.925 9.05779 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 135.645 117.77779 \nL 135.645 9.05779 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 117.77779 \nL 135.645 117.77779 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 9.05779 \nL 135.645 9.05779 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p330d78e773\">\n   <rect height=\"108.72\" width=\"108.72\" x=\"26.925\" y=\"9.05779\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(train_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polished-accuracy",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "정답 : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"정답 : %d\" %train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "copyrighted-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = {}\n",
    "class_name[0] = 'sandal'\n",
    "class_name[1] = 'sneaker'\n",
    "class_name[2] = 'ankle boot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "introductory-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 셔플\n",
    "index = np.array(range(len(train_image)))\n",
    "np.random.shuffle(index)\n",
    "train_image = train_image[index]\n",
    "train_label = train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "motivated-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 : 필요하다면 원하는대로 수정할 것. (단 openCV등 패키지는 사용불가하고, 넘파이만 이용해서 전처리할 것.)\n",
    "train_image = (train_image -[128]) / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pursuant-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터와 검증데이터 분리 \n",
    "sp = int(1e4)  # 원하는 숫자로 변경 가능.\n",
    "train_x = train_image[:sp]\n",
    "train_y = train_label[:sp].astype(np.int32)\n",
    "valid_x = train_image[sp:]\n",
    "valid_y = train_label[sp:].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "renewable-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = train_x[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stuffed-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토치 텐서로 변환\n",
    "train_x = torch.FloatTensor(train_x).view(-1,1, 28,28)\n",
    "train_y = torch.LongTensor(train_y)\n",
    "valid_x = torch.FloatTensor(valid_x).view(-1,1,28,28)\n",
    "valid_y = torch.LongTensor(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "heard-swimming",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13.0 15.5 61.0\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000, 32, 13, 13])\n",
      "torch.Size([10000, 64, 6, 6])\n",
      "torch.Size([10000, 32, 3, 3])\n",
      "torch.Size([96000, 30])\n",
      "torch.Size([96000, 3])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (96000) must match the size of tensor b (10000) at non-singleton dimension 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f64291a37b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-f64291a37b8f>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, input_data, target_data)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mcorrect_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect_cnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (96000) must match the size of tensor b (10000) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "class net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        \n",
    "        #input (num_batch, channel=1, 28, 28)\n",
    "        #생각한 신경망 모델로 변환할 것. (Conv2d, Linear 조합 이용.)\n",
    "        self.conv1  = nn.Conv2d(1, 32, kernel_size = 4, stride=2, padding=0)\n",
    "        size1 = (28+2*0-4)/2+1\n",
    "        self.conv2  = nn.Conv2d(32, 64, kernel_size = 3, stride=2, padding=0)\n",
    "        size2 = (32+2*0-3)/2+1\n",
    "        self.conv3  = nn.Conv2d(64, 32, kernel_size = 4, stride=1, padding=0)\n",
    "        size3 = (64+2*0-4)/1+1\n",
    "        self.fc     = nn.Linear(30, 3)\n",
    "        print(size1, size2, size3)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x): #구현한 신경망 모델에 따라 적절히 변형할 것\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, 30)\n",
    "        print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, image): #test image는 넘파이 (28,28) 형상 2d 배열임. 테스트 이미지 1장을 입력했을 때 결과가 출력되도록 할 것.\n",
    "        image = (image -[128])/128\n",
    "        image = torch.from_numpy(image).float()\n",
    "        image = image.view(-1,1, 28,28)\n",
    "        out   = self.forward(image).view(-1)\n",
    "        return out.argmax().item()\n",
    "    \n",
    "    def accuracy(self, input_data, target_data):\n",
    "        predict = self.forward(input_data).argmax(axis=1)\n",
    "        correct_cnt = sum(predict==target_data)\n",
    "        return correct_cnt.item()/len(target_data)\n",
    "    \n",
    "    def save(self, name):\n",
    "        torch.save(self.state_dict(), name + '.pt')\n",
    "        print('Models saved successfully')\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.load_state_dict(torch.load(name + '.pt'))\n",
    "        print ('Models loaded succesfully')\n",
    "    \n",
    "cnn = net()\n",
    "cnn.accuracy(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "lesser-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "stable-least",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13.0\n",
      "6.0\n",
      "3\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3840000) must match the size of tensor b (10000) at non-singleton dimension 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6fed66421f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-292d3c82a8e7>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, input_data, target_data)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mcorrect_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect_cnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3840000) must match the size of tensor b (10000) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "cnn.accuracy(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "classified-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    \n",
    "    def __init__(self, net, lr, figname, train_x, train_y, valid_x, valid_y):\n",
    "        \n",
    "        self.net = net\n",
    "        self.lrate = lr\n",
    "        self.name = figname\n",
    "        self.train_x, self.train_y = train_x, train_y\n",
    "        self.valid_x, self.valid_y = valid_x, valid_y\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lrate)\n",
    "        self.loss_layer    = nn.CrossEntropyLoss()\n",
    "        self.train_loss_history = [] \n",
    "        self.valid_acc_history = [] \n",
    "    \n",
    "    def plot(self, train_loss, valid_acc):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(121)\n",
    "        plt.title('train loss')\n",
    "        plt.plot(train_loss, 'k-')\n",
    "        plt.subplot(122)\n",
    "        plt.title('valid acc')\n",
    "        plt.plot(valid_acc)\n",
    "        plt.savefig(self.name +'.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self, epoch=10):\n",
    "        \n",
    "        for j in range(epoch):\n",
    "            \n",
    "            train_loss = 0\n",
    "            idx = np.array(range(sp))\n",
    "            np.random.shuffle(idx)\n",
    "            train_x = self.train_x[idx]\n",
    "            train_y = self.train_y[idx]\n",
    "            batch_size = 500 # 필요한 경우 변경\n",
    "            for k in range(len(train_x)//batch_size): #필요한 경우 레인지 내 변수 변경\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_layer(self.net.forward(train_x[batch_size*k:batch_size*(k+1)]), train_y[batch_size*k:batch_size*(k+1)])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                \n",
    "            self.train_loss_history.append(train_loss)\n",
    "            self.valid_acc_history.append(self.net.accuracy(valid_x, valid_y))\n",
    "            \n",
    "            self.plot(self.train_loss_history, self.valid_acc_history)\n",
    "            if np.argmax(self.valid_acc_history) == j:\n",
    "                print(\"%d-th epoch에서 진척이 있었음.\" %(j+1))\n",
    "                savename = 'cnn' + self.name\n",
    "                self.net.save(savename)\n",
    "            else:\n",
    "                print(\"%d-th epoch에서는 진척 없음.\" %(j+1))\n",
    "            print(\"현재 검증 데이터의 분류 성공율 최댓값은 %.3f%%\" %(100*np.max(self.valid_acc_history)))\n",
    "        print(\"훈련 종료.\")\n",
    "        print(\"%d번째 epoch에서 검증 정답율 최대\" %(np.argmax(self.valid_acc_history)+1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "republican-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 학습율로 변경\n",
    "tt = trainer(cnn, 1e-3, 'lr1e-3', train_x, train_y, valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "statewide-advantage",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 30]' is invalid for input of size 2704000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9fea3d91633d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 원하는 회수로 변경\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-2d124c197725>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c6dee7820a36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#구현한 신경망 모델에 따라 적절히 변형할 것\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 30]' is invalid for input of size 2704000"
     ]
    }
   ],
   "source": [
    "# 원하는 회수로 변경\n",
    "tt.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "executed-latino",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 30]' is invalid for input of size 54080000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6fed66421f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-c6dee7820a36>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, input_data, target_data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mcorrect_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect_cnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c6dee7820a36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#구현한 신경망 모델에 따라 적절히 변형할 것\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 30]' is invalid for input of size 54080000"
     ]
    }
   ],
   "source": [
    "cnn.accuracy(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.accuracy(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "guided-continuity",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for net:\n\tMissing key(s) in state_dict: \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([30, 1, 28, 28]) from checkpoint, the shape in current model is torch.Size([32, 1, 4, 4]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([3, 30]) from checkpoint, the shape in current model is torch.Size([3, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-91f012b5b5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 제출한 세이브파일을 로드할 수 있도록 파일 네임을 입력해서 제출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnnlr1e-3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c6dee7820a36>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Models loaded succesfully'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1224\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for net:\n\tMissing key(s) in state_dict: \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([30, 1, 28, 28]) from checkpoint, the shape in current model is torch.Size([32, 1, 4, 4]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([3, 30]) from checkpoint, the shape in current model is torch.Size([3, 2])."
     ]
    }
   ],
   "source": [
    "# 제출한 세이브파일을 로드할 수 있도록 파일 네임을 입력해서 제출\n",
    "cnn.load('cnnlr1e-3') \n",
    "cnn.accuracy(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "considered-salvation",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hw_test_image.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-02b0be15b0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#이 부분이 실행되지 않으면 채점불가하니, 아래 부분이 실행될 수 있도록 과제를 제출 할 것.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# test data는 개수만 다를 뿐 초기형태는 훈련데이터와 동일한 (28,28) 2d 넘파이 배열임.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hw_test_image.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hw_test_label.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mctr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML38/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hw_test_image.npy'"
     ]
    }
   ],
   "source": [
    "# 아래 부분을 채점을 위한 부분임. \n",
    "# 아래 부분은 변경불가!\n",
    "#이 부분이 실행되지 않으면 채점불가하니, 아래 부분이 실행될 수 있도록 과제를 제출 할 것. \n",
    "# test data는 개수만 다를 뿐 초기형태는 훈련데이터와 동일한 (28,28) 2d 넘파이 배열임.\n",
    "test_image = np.load('hw_test_image.npy')\n",
    "test_label = np.load('hw_test_label.npy')\n",
    "ctr = 0\n",
    "for i, img in enumerate(test_image):\n",
    "    guess = cnn.predict(img)\n",
    "    if guess == test_label[i]:\n",
    "        ctr +=1\n",
    "print(\"테스트 데이터 분류 성공율 : %.2f%%\" %(100*ctr/len(test_label)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-slope",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd01aedf9346abdc7ffe4a05be72f7b7e546e7cd227fb35de4fc67c2c00df38b1b7",
   "display_name": "Python 3.8.8 64-bit ('ML38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}