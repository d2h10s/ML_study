{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thousand-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monetary-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    " # MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software. #\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit fashion mnist 데이터 셋의 일부를 약간 변형하여 만든 과제를 위한 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cathedral-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow keras dataset에서 가져온 데이타셋.\n",
    "train_image = np.load('hw_train_image.npy')\n",
    "train_label = np.load('hw_train_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boxed-humanity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000,)\n",
      "(18000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_label.shape)\n",
    "print(train_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legislative-textbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(train_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polished-accuracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답 : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"정답 : %d\" %train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "copyrighted-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = {}\n",
    "class_name[0] = 'sandal'\n",
    "class_name[1] = 'sneaker'\n",
    "class_name[2] = 'ankle boot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "introductory-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 셔플\n",
    "index = np.array(range(len(train_image)))\n",
    "np.random.shuffle(index)\n",
    "train_image = train_image[index]\n",
    "train_label = train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "motivated-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 : 필요하다면 원하는대로 수정할 것. (단 openCV등 패키지는 사용불가하고, 넘파이만 이용해서 전처리할 것.)\n",
    "train_image = (train_image -[128]) / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pursuant-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터와 검증데이터 분리 \n",
    "sp = int(12e3)  # 원하는 숫자로 변경 가능.\n",
    "train_x = train_image[:sp]\n",
    "train_y = train_label[:sp].astype(np.int32)\n",
    "valid_x = train_image[sp:]\n",
    "valid_y = train_label[sp:].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "renewable-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = train_x[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stuffed-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토치 텐서로 변환\n",
    "train_x = torch.FloatTensor(train_x).view(-1,1, 28,28)\n",
    "train_y = torch.LongTensor(train_y)\n",
    "valid_x = torch.FloatTensor(valid_x).view(-1,1,28,28)\n",
    "valid_y = torch.LongTensor(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "heard-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        \n",
    "        #input (num_batch, channel=1, 28, 28)\n",
    "        #생각한 신경망 모델로 변환할 것. (Conv2d, Linear 조합 이용.)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 4, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 4, stride=1, padding=0)\n",
    "        self.fc    = nn.Linear(128, 3) \n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x): #구현한 신경망 모델에 따라 적절히 변형할 것\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, image): #test image는 넘파이 (28,28) 형상 2d 배열임. 테스트 이미지 1장을 입력했을 때 결과가 출력되도록 할 것.\n",
    "        image = (image -[128])/128\n",
    "        image = torch.from_numpy(image).float()\n",
    "        image = image.view(-1,1, 28,28)\n",
    "        out   = self.forward(image).view(-1)\n",
    "        return out.argmax().item()\n",
    "    \n",
    "    def accuracy(self, input_data, target_data):\n",
    "        predict = self.forward(input_data).argmax(axis=1)\n",
    "        correct_cnt = sum(predict==target_data)\n",
    "        return correct_cnt.item()/len(target_data)\n",
    "    \n",
    "    def save(self, name):\n",
    "        torch.save(self.state_dict(), name + '.pt')\n",
    "        print('Models saved successfully')\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.load_state_dict(torch.load(name + '.pt'))\n",
    "        print ('Models loaded succesfully')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lesser-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stable-least",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4332000) must match the size of tensor b (12000) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6fed66421f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b42f33ba129c>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, input_data, target_data)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mcorrect_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect_cnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4332000) must match the size of tensor b (12000) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "cnn.accuracy(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    \n",
    "    def __init__(self, net, lr, figname, train_x, train_y, valid_x, valid_y):\n",
    "        \n",
    "        self.net = net\n",
    "        self.lrate = lr\n",
    "        self.name = figname\n",
    "        self.train_x, self.train_y = train_x, train_y\n",
    "        self.valid_x, self.valid_y = valid_x, valid_y\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lrate)\n",
    "        self.loss_layer    = nn.CrossEntropyLoss()\n",
    "        self.train_loss_history = [] \n",
    "        self.valid_acc_history = [] \n",
    "    \n",
    "    def plot(self, train_loss, valid_acc):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(121)\n",
    "        plt.title('train loss')\n",
    "        plt.plot(train_loss, 'k-')\n",
    "        plt.subplot(122)\n",
    "        plt.title('valid acc')\n",
    "        plt.plot(valid_acc)\n",
    "        plt.savefig(self.name +'.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self, epoch=10):\n",
    "        \n",
    "        for j in range(epoch):\n",
    "            \n",
    "            train_loss = 0\n",
    "            idx = np.array(range(sp))\n",
    "            np.random.shuffle(idx)\n",
    "            train_x = self.train_x[idx]\n",
    "            train_y = self.train_y[idx]\n",
    "            batch_size = 500 # 필요한 경우 변경\n",
    "            for k in range(len(train_x)//batch_size): #필요한 경우 레인지 내 변수 변경\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_layer(self.net.forward(train_x[batch_size*k:batch_size*(k+1)]), train_y[batch_size*k:batch_size*(k+1)])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                \n",
    "            self.train_loss_history.append(train_loss)\n",
    "            self.valid_acc_history.append(self.net.accuracy(valid_x, valid_y))\n",
    "            \n",
    "            self.plot(self.train_loss_history, self.valid_acc_history)\n",
    "            if np.argmax(self.valid_acc_history) == j:\n",
    "                print(\"%d-th epoch에서 진척이 있었음.\" %(j+1))\n",
    "                savename = 'cnn' + self.name\n",
    "                self.net.save(savename)\n",
    "            else:\n",
    "                print(\"%d-th epoch에서는 진척 없음.\" %(j+1))\n",
    "            print(\"현재 검증 데이터의 분류 성공율 최댓값은 %.3f%%\" %(100*np.max(self.valid_acc_history)))\n",
    "        print(\"훈련 종료.\")\n",
    "        print(\"%d번째 epoch에서 검증 정답율 최대\" %(np.argmax(self.valid_acc_history)+1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 학습율로 변경\n",
    "tt = trainer(cnn, 1e-3, 'lr1e-3', train_x, train_y, valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 회수로 변경\n",
    "tt.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.accuracy(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.accuracy(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출한 세이브파일을 로드할 수 있도록 파일 네임을 입력해서 제출\n",
    "cnn.load('cnnlr1e-3') \n",
    "cnn.accuracy(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8c749",
   "metadata": {},
   "source": [
    "# 아래 부분을 채점을 위한 부분임. \n",
    "# 아래 부분은 변경불가!\n",
    "#이 부분이 실행되지 않으면 채점불가하니, 아래 부분이 실행될 수 있도록 과제를 제출 할 것. \n",
    "# test data는 개수만 다를 뿐 초기형태는 훈련데이터와 동일한 (28,28) 2d 넘파이 배열임.\n",
    "test_image = np.load('hw_test_image.npy')\n",
    "test_label = np.load('hw_test_label.npy')\n",
    "ctr = 0\n",
    "for i, img in enumerate(test_image):\n",
    "    guess = cnn.predict(img)\n",
    "    if guess == test_label[i]:\n",
    "        ctr +=1\n",
    "print(\"테스트 데이터 분류 성공율 : %.2f%%\" %(100*ctr/len(test_label)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-slope",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
