{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0a61fc779d08deaee957108e0966e05422a283b1c762ee0e088310fcc993d6c21",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상수 정의\n",
    "ENV = 'Acrobot-v1'  # 태스크 이름\n",
    "GAMMA = 0.99  # 시간할인율\n",
    "MAX_STEPS = 200  # 1에피소드 당 최대 단계 수\n",
    "NUM_EPISODES = 1000  # 최대 에피소드 수\n",
    "\n",
    "NUM_PROCESSES = 32  # 동시 실행 환경 수\n",
    "NUM_ADVANCED_STEP = 5  # 총 보상을 계산할 때 Advantage 학습을 할 단계 수\n",
    "\n",
    "# A2C 손실함수 계산에 사용되는 상수\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "max_grad_norm = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    '''Advantage 학습에 사용할 메모리 클래스'''\n",
    "\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\n",
    "\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, 6)\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
    "\n",
    "        # 할인 총보상 저장\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.index = 0  # insert할 인덱스\n",
    "\n",
    "    def insert(self, current_obs, action, reward, mask):\n",
    "        '''현재 인덱스 위치에 transition을 저장'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 값 업데이트\n",
    "\n",
    "    def after_update(self):\n",
    "        '''Advantage학습 단계만큼 단계가 진행되면 가장 새로운 transition을 index0에 저장'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value):\n",
    "        '''Advantage학습 범위 안의 각 단계에 대해 할인 총보상을 계산'''\n",
    "\n",
    "        # 주의 : 5번째 단계부터 거슬러 올라오며 계산\n",
    "        # 주의 : 5번째 단계가 Advantage1, 4번째 단계는 Advantage2가 됨\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * \\\n",
    "                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.actor = nn.Linear(n_mid, n_out)\n",
    "        self.critic = nn.Linear(n_mid, 1)\n",
    "    \n",
    "    # 신경망의 순전파\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        critic_output = self.critic(h2)\n",
    "        actor_output = self.actor(h2)\n",
    "\n",
    "        return critic_output, actor_output\n",
    "    \n",
    "    def act(self, x):\n",
    "        value, actor_output = self(x)\n",
    "        action_probs = F.softmax(actor_output, dim=1)\n",
    "        action = action_probs.multinomial(num_samples=1) # 샘플링\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, x, actions):\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(actor_output, dim=1)\n",
    "        action_log_probs = log_probs.gather(1, actions)\n",
    "\n",
    "        probs = F.softmax(actor_output, dim=1)\n",
    "        entropy = -(log_probs * probs).sum(-1).mean()\n",
    "\n",
    "        return value, action_log_probs, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트의 두뇌 역할을 하는 클래스. 모든 에이전트가 공유한다\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class Brain(object):\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic  # actor_critic은 Net 클래스로 구현한 신경망\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)\n",
    "\n",
    "    def update(self, rollouts):\n",
    "        '''Advantage학습의 대상이 되는 5단계 모두를 사용하여 수정'''\n",
    "        obs_shape = rollouts.observations.size()[2:]  # torch.Size([4, 84, 84])\n",
    "        num_steps = NUM_ADVANCED_STEP\n",
    "        num_processes = NUM_PROCESSES\n",
    "\n",
    "        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n",
    "            rollouts.observations[:-1].view(-1, 4),\n",
    "            rollouts.actions.view(-1, 1))\n",
    "\n",
    "        # 주의 : 각 변수의 크기\n",
    "        # rollouts.observations[:-1].view(-1, 4) torch.Size([80, 4])\n",
    "        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n",
    "        # values torch.Size([80, 1])\n",
    "        # action_log_probs torch.Size([80, 1])\n",
    "        # entropy torch.Size([])\n",
    "\n",
    "        values = values.view(num_steps, num_processes,\n",
    "                             1)  # torch.Size([5, 16, 1])\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        # advantage(행동가치-상태가치) 계산\n",
    "        advantages = rollouts.returns[:-1] - values  # torch.Size([5, 16, 1])\n",
    "\n",
    "        # Critic의 loss 계산\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # Actor의 gain 계산, 나중에 -1을 곱하면 loss가 된다\n",
    "        action_gain = (action_log_probs*advantages.detach()).mean()\n",
    "        # detach 메서드를 호출하여 advantages를 상수로 취급\n",
    "\n",
    "        # 오차함수의 총합\n",
    "        total_loss = (value_loss * value_loss_coef -\n",
    "                      action_gain - entropy * entropy_coef)\n",
    "\n",
    "        # 결합 가중치 수정\n",
    "        self.actor_critic.train()  # 신경망을 학습 모드로 전환\n",
    "        self.optimizer.zero_grad()  # 경사를 초기화\n",
    "        total_loss.backward()  # 역전파 계산\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n",
    "        # 결합 가중치가 한번에 너무 크게 변화하지 않도록, 경사를 0.5 이하로 제한함(클리핑)\n",
    "\n",
    "        self.optimizer.step()  # 결합 가중치 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def run(self):\n",
    "        '''실행 엔트리 포인트'''\n",
    "\n",
    "        # 동시 실행할 환경 수 만큼 env를 생성\n",
    "        envs = [gym.make(ENV) for i in range(NUM_PROCESSES)]\n",
    "\n",
    "        # 모든 에이전트가 공유하는 Brain 객체를 생성\n",
    "        n_in = envs[0].observation_space.shape[0]  # 상태 변수 수는 4\n",
    "        n_out = envs[0].action_space.n  # 행동 가짓수는 2\n",
    "        n_mid = 32\n",
    "        actor_critic = Net(n_in, n_mid, n_out)  # 신경망 객체 생성\n",
    "        global_brain = Brain(actor_critic)\n",
    "\n",
    "        # 각종 정보를 저장하는 변수\n",
    "        obs_shape = n_in\n",
    "        current_obs = torch.zeros(\n",
    "            NUM_PROCESSES, obs_shape)  # torch.Size([16, 4])\n",
    "        rollouts = RolloutStorage(\n",
    "            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)  # rollouts 객체\n",
    "        episode_rewards = torch.zeros([NUM_PROCESSES, 1])  # 현재 에피소드의 보상\n",
    "        final_rewards = torch.zeros([NUM_PROCESSES, 1])  # 마지막 에피소드의 보상\n",
    "        obs_np = np.zeros([NUM_PROCESSES, obs_shape])  # Numpy 배열\n",
    "        reward_np = np.zeros([NUM_PROCESSES, 1])  # Numpy 배열\n",
    "        done_np = np.zeros([NUM_PROCESSES, 1])  # Numpy 배열\n",
    "        each_step = np.zeros(NUM_PROCESSES)  # 각 환경의 단계 수를 기록\n",
    "        episode = 0  # 환경 0의 에피소드 수\n",
    "\n",
    "        # 초기 상태로부터 시작\n",
    "        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n",
    "        obs = np.array(obs)\n",
    "        obs = torch.from_numpy(obs).float()  # torch.Size([16, 4])\n",
    "        current_obs = obs  # 가장 최근의 obs를 저장\n",
    "        \n",
    "        # advanced 학습에 사용되는 객체 rollouts 첫번째 상태에 현재 상태를 저장\n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "        # 1 에피소드에 해당하는 반복문\n",
    "        for j in range(NUM_EPISODES*NUM_PROCESSES):  # 전체 for문\n",
    "            # advanced 학습 대상이 되는 각 단계에 대해 계산\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "\n",
    "                # 행동을 선택\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "\n",
    "                # (16,1)→(16,) -> tensor를 NumPy변수로\n",
    "                actions = action.squeeze(1).numpy()\n",
    "\n",
    "                # 한 단계를 실행\n",
    "                for i in range(NUM_PROCESSES):\n",
    "                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(\n",
    "                        actions[i])\n",
    "\n",
    "                    # episode의 종료가치, state_next를 설정\n",
    "                    if done_np[i]:  # 단계 수가 200을 넘거나, 봉이 일정 각도 이상 기울면 done이 True가 됨\n",
    "\n",
    "                        # 환경 0일 경우에만 출력\n",
    "                        if i == 0:\n",
    "                            print('%d Episode: Finished after %d steps' % (\n",
    "                                episode, each_step[i]+1))\n",
    "                            episode += 1\n",
    "\n",
    "                        # 보상 부여\n",
    "                        if each_step[i] < 195:\n",
    "                            reward_np[i] = -1.0  # 도중에 봉이 넘어지면 페널티로 보상 -1 부여\n",
    "                        else:\n",
    "                            reward_np[i] = 1.0  # 봉이 쓰러지지 않고 끝나면 보상 1 부여\n",
    "\n",
    "                        each_step[i] = 0  # 단계 수 초기화\n",
    "                        obs_np[i] = envs[i].reset()  # 실행 환경 초기화\n",
    "\n",
    "                    else:\n",
    "                        reward_np[i] = 0.0  # 그 외의 경우는 보상 0 부여\n",
    "                        each_step[i] += 1\n",
    "\n",
    "                # 보상을 tensor로 변환하고, 에피소드의 총보상에 더해줌\n",
    "                reward = torch.from_numpy(reward_np).float()\n",
    "                episode_rewards += reward\n",
    "\n",
    "                # 각 실행 환경을 확인하여 done이 true이면 mask를 0으로, false이면 mask를 1로\n",
    "                masks = torch.FloatTensor(\n",
    "                    [[0.0] if done_ else [1.0] for done_ in done_np])\n",
    "\n",
    "                # 마지막 에피소드의 총 보상을 업데이트\n",
    "                final_rewards *= masks  # done이 false이면 1을 곱하고, true이면 0을 곱해 초기화\n",
    "                # done이 false이면 0을 더하고, true이면 episode_rewards를 더해줌\n",
    "                final_rewards += (1 - masks) * episode_rewards\n",
    "\n",
    "                # 에피소드의 총보상을 업데이트\n",
    "                episode_rewards *= masks  # done이 false인 에피소드의 mask는 1이므로 그대로, true이면 0이 됨\n",
    "\n",
    "                # 현재 done이 true이면 모두 0으로 \n",
    "                current_obs *= masks\n",
    "\n",
    "                # current_obs를 업데이트\n",
    "                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\n",
    "                current_obs = obs  # 최신 상태의 obs를 저장\n",
    "\n",
    "                # 메모리 객체에 현 단계의 transition을 저장\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "\n",
    "            # advanced 학습 for문 끝\n",
    "\n",
    "            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(\n",
    "                    rollouts.observations[-1]).detach()\n",
    "                # rollouts.observations의 크기는 torch.Size([6, 16, 4])\n",
    "\n",
    "            # 모든 단계의 할인총보상을 계산하고, rollouts의 변수 returns를 업데이트\n",
    "            rollouts.compute_returns(next_value)\n",
    "\n",
    "            # 신경망 및 rollout 업데이트\n",
    "            global_brain.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "\n",
    "            # 환경 갯수를 넘어서는 횟수로 200단계를 버텨내면 성공\n",
    "            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n",
    "                print('연속성공')\n",
    "                break"
   ]
  },
  {
   "source": [
    "env = Environment()\n",
    "env.run()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 91,
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (240x4 and 6x32)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-33a17a339531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-50ca4ab164c4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# 신경망 및 rollout 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mglobal_brain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-835d4a98755a>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnum_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_PROCESSES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             rollouts.actions.view(-1, 1))\n",
      "\u001b[0;32m<ipython-input-88-8bfc806e729e>\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, x, actions)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-8bfc806e729e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 신경망의 순전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcritic_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (240x4 and 6x32)"
     ]
    }
   ]
  }
 ]
}