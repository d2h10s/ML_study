{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 16 – Natural Language Processing with RNNs and Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code in chapter 16._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q -U tensorflow-addons\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting a sequence into batches of shuffled windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., `[2, 3, 4, 5, 6]` would be split into `[[2, 3, 4, 5], [3, 4, 5, 6]]`), then create batches of 3 such input/target pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31370/31370 [==============================] - 601s 19ms/step - loss: 1.6220\n",
      "Epoch 2/10\n",
      "31370/31370 [==============================] - 604s 19ms/step - loss: 1.5340\n",
      "Epoch 3/10\n",
      "31370/31370 [==============================] - 601s 19ms/step - loss: 1.5124\n",
      "Epoch 4/10\n",
      "25871/31370 [=======================>......] - ETA: 1:45 - loss: 1.5010"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-f85cbe487a4c>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the countellance and the belly and the good one str\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toke ob our farteness parce. trust you, tridures al\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpeniomem lvcer togmaze:\n",
      "yel 'vall dear. ruli-hapem\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.repeat().batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, #recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 2.6212\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 2.2411\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 2.1109\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 2.0348\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.9846\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.9480\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.9204\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8988\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8802\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8673\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.8538\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.8418\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8329\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8234\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8163\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8085\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.8034\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7988\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7893\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7865\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7822\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7774\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7747\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7695\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7693\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7635\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7601\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7586\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7541\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7538\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7494\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7463\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7421\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7417\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7396\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7373\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7377\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7333\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7302\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7303\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7274\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7266\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7274\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7255\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7236\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7220\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7203\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7205\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7165\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7180\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model with different batch sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the weights, we first need to build the model (so the weights get created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ty:\n",
      "no doing honour this vows wan. believe yourself\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the IMDB dataset easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'unsupervised'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5305 - accuracy: 0.7281\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3459 - accuracy: 0.8554\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.1913 - accuracy: 0.9319\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.1341 - accuracy: 0.9535\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.1010 - accuracy: 0.9624\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using manual masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5426 - accuracy: 0.7155\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.3477 - accuracy: 0.8555\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.1749 - accuracy: 0.9370\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.1268 - accuracy: 0.9538\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.1069 - accuracy: 0.9599\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "embed_size = 128\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 4s 5ms/step - loss: 0.5460 - accuracy: 0.7267\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 4s 5ms/step - loss: 0.5129 - accuracy: 0.7494\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 4s 5ms/step - loss: 0.5082 - accuracy: 0.7530\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 4s 5ms/step - loss: 0.5046 - accuracy: 0.7538\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 4s 5ms/step - loss: 0.5017 - accuracy: 0.7561\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의**: 이 섹션의 코드는 텐서플로 애드온 0.10.0에서 에러가 발생합니다. 0.9.1 버전을 설치하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons==0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embed_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)#, training=None)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 4.6052\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 4.6026\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Recurrent Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, None, 10)          660       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 20)          1320      \n",
      "=================================================================\n",
      "Total params: 1,980\n",
      "Trainable params: 1,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 201\n",
    "max_dims = 512\n",
    "pos_emb = PositionalEncoding(max_steps, max_dims)\n",
    "PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i1, i2, crop_i = 100, 101, 150\n",
    "p1, p2, p3 = 22, 60, 35\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
    "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
    "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
    "ax1.plot(p3, PE[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
    "ax1.plot(PE[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
    "ax1.plot(PE[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
    "ax1.plot([p1, p2], [PE[p1, i1], PE[p2, i1]], \"bo\")\n",
    "ax1.plot([p1, p2], [PE[p1, i2], PE[p2, i2]], \"ro\")\n",
    "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
    "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.hlines(0, 0, max_steps - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
    "ax1.axis([0, max_steps - 1, -1, 1])\n",
    "ax2.imshow(PE.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
    "ax2.hlines(i1, 0, max_steps - 1, color=\"b\")\n",
    "cheat = 2 # need to raise the red line a bit, or else it hides the blue one\n",
    "ax2.hlines(i2+cheat, 0, max_steps - 1, color=\"r\")\n",
    "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
    "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
    "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
    "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
    "ax2.axis([0, max_steps - 1, 0, crop_i])\n",
    "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
    "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
    "plt.savefig(\"positional_embedding_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a (very) simplified Transformer (the actual architecture has skip connections, layer norm, dense nets, and most importantly it uses Multi-Head Attention instead of regular Attention):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "outputs = keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic implementation of the `MultiHeadAttention` layer. One will likely be added to `keras.layers` in the near future. Note that `Conv1D` layers with `kernel_size=1` (and the default `padding=\"valid\"` and `strides=1`) is equivalent to a `TimeDistributed(Dense(...))` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self, n_heads, causal=False, use_scale=False, **kwargs):\n",
    "        self.n_heads = n_heads\n",
    "        self.causal = causal\n",
    "        self.use_scale = use_scale\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.dims = batch_input_shape[0][-1]\n",
    "        self.q_dims, self.v_dims, self.k_dims = [self.dims // self.n_heads] * 3 # could be hyperparameters instead\n",
    "        self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size=1, use_bias=False)\n",
    "        self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size=1, use_bias=False)\n",
    "        self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size=1, use_bias=False)\n",
    "        self.attention = keras.layers.Attention(causal=self.causal, use_scale=self.use_scale)\n",
    "        self.out_linear = keras.layers.Conv1D(self.dims, kernel_size=1, use_bias=False)\n",
    "        super().build(batch_input_shape)\n",
    "    def _multi_head_linear(self, inputs, linear):\n",
    "        shape = K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]])\n",
    "        projected = K.reshape(linear(inputs), shape)\n",
    "        perm = K.permute_dimensions(projected, [0, 2, 1, 3])\n",
    "        return K.reshape(perm, [shape[0] * self.n_heads, shape[1], -1])\n",
    "    def call(self, inputs):\n",
    "        q = inputs[0]\n",
    "        v = inputs[1]\n",
    "        k = inputs[2] if len(inputs) > 2 else v\n",
    "        shape = K.shape(q)\n",
    "        q_proj = self._multi_head_linear(q, self.q_linear)\n",
    "        v_proj = self._multi_head_linear(v, self.v_linear)\n",
    "        k_proj = self._multi_head_linear(k, self.k_linear)\n",
    "        multi_attended = self.attention([q_proj, v_proj, k_proj])\n",
    "        shape_attended = K.shape(multi_attended)\n",
    "        reshaped_attended = K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]])\n",
    "        perm = K.permute_dimensions(reshaped_attended, [0, 2, 1, 3])\n",
    "        concat = K.reshape(perm, [shape[0], shape_attended[1], -1])\n",
    "        return self.out_linear(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer multi_head_attention is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer multi_head_attention is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 50, 512])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.random.rand(2, 50, 512)\n",
    "V = np.random.rand(2, 80, 512)\n",
    "multi_attn = MultiHeadAttention(8)\n",
    "multi_attn([Q, V]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "_Exercise:_ Embedded Reber grammars _were used by Hochreiter and Schmidhuber in [their paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce strings such as \"BPBTSXXVPSEPE.\" Check out Jenny Orr's [nice introduction](https://homl.info/108) to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr's page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don't._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to build a function that generates strings based on a grammar. The grammar will be represented as a list of possible transitions for each state. A transition specifies the string to output (or a grammar to generate it) and the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
    "\n",
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]]\n",
    "\n",
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a few strings based on the default Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's generate a few strings based on the embedded Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we need a function to generate strings that do not respect the grammar. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "\n",
    "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few corrupted strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot feed strings directly to an RNN, so we need to encode them somehow. One option would be to one-hot encode each character. Another option is to use embeddings. Let's go for the second option (but since there are just a handful of characters, one-hot encoding would probably be a good option as well). For embeddings to work, we need to convert each string into a sequence of character IDs. Let's write a function for that, using each character's index in the string of possible characters \"BEPSTVX\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
    "    return [POSSIBLE_CHARS.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids(\"BTTTXXVVETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate the dataset, with 50% good strings, and 50% bad strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    good_strings = [string_to_ids(generate_string(embedded_reber_grammar))\n",
    "                    for _ in range(size // 2)]\n",
    "    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
    "                   for _ in range(size - size // 2)]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
    "                 [[0.] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first training sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(22,), dtype=int32, numpy=\n",
       "array([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What classes does it belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6910 - accuracy: 0.5095 - val_loss: 0.6825 - val_accuracy: 0.5645\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6678 - accuracy: 0.5659 - val_loss: 0.6635 - val_accuracy: 0.6105\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6504 - accuracy: 0.5766 - val_loss: 0.6521 - val_accuracy: 0.6110\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6347 - accuracy: 0.5980 - val_loss: 0.6224 - val_accuracy: 0.6445\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6054 - accuracy: 0.6361 - val_loss: 0.5779 - val_accuracy: 0.6980\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5414 - accuracy: 0.7093 - val_loss: 0.4695 - val_accuracy: 0.7795\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3809 - accuracy: 0.8390 - val_loss: 0.6045 - val_accuracy: 0.6225\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4180 - accuracy: 0.8051 - val_loss: 0.4378 - val_accuracy: 0.7345\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2235 - accuracy: 0.9216 - val_loss: 0.1663 - val_accuracy: 0.9655\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1336 - accuracy: 0.9609 - val_loss: 0.1330 - val_accuracy: 0.9615\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0451 - accuracy: 0.9899 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.0085 - val_accuracy: 0.9985\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 8.6119e-04 - accuracy: 1.0000 - val_loss: 5.2998e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 4.5493e-04 - accuracy: 1.0000 - val_loss: 4.0827e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 3.6301e-04 - accuracy: 1.0000 - val_loss: 3.3593e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 3.0536e-04 - accuracy: 1.0000 - val_loss: 2.8693e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 2.6468e-04 - accuracy: 1.0000 - val_loss: 2.5131e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 2.3439e-04 - accuracy: 1.0000 - val_loss: 2.2378e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 2.1034e-04 - accuracy: 1.0000 - val_loss: 2.0217e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "embedding_size = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum = 0.95, nesterov=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# TF 2.2 & cuDNN 7.6.5에서 에러가 발생하기 때문에 CPU로 실행합니다.\n",
    "with tf.device('/CPU'):\n",
    "    history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character. If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter. That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated probability that these are Reber strings:\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.06%\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.97%\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
    "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n",
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
    "\n",
    "# TF 2.2 & cuDNN 7.6.5에서 에러가 발생하기 때문에 CPU로 실행합니다.\n",
    "with tf.device('/CPU'):\n",
    "    y_proba = model.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(\"Estimated probability that these are Reber strings:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta-da! It worked fine. The RNN found the correct answers with very high confidence. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "_Exercise: Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few random dates, displayed in both the input format and the target format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the list of all possible characters in the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADFJMNOSabceghilmnoprstuvy01234567890, '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"01234567890, \"\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the list of possible characters in the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to convert a string to a list of character IDs, as we did in the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 11, 19, 22, 11, 16, 9, 11, 20, 38, 28, 26, 37, 38, 33, 26, 33, 31]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First version: a very basic seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
    "\n",
    "Since the decoder expects a sequence as input, we repeat the vector (which is output by the decoder) as many times as the longest possible output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7985 - accuracy: 0.3575 - val_loss: 1.3444 - val_accuracy: 0.5033\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.4000 - accuracy: 0.5000 - val_loss: 1.6329 - val_accuracy: 0.4155\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.1020 - accuracy: 0.6110 - val_loss: 0.9927 - val_accuracy: 0.6363\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.7934 - accuracy: 0.7056 - val_loss: 0.6830 - val_accuracy: 0.7408\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.8209 - accuracy: 0.7061 - val_loss: 0.8538 - val_accuracy: 0.6841\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.7185 - accuracy: 0.7416 - val_loss: 0.4656 - val_accuracy: 0.8170\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3817 - accuracy: 0.8525 - val_loss: 0.3208 - val_accuracy: 0.8737\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3764 - accuracy: 0.8722 - val_loss: 0.2402 - val_accuracy: 0.9180\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1838 - accuracy: 0.9462 - val_loss: 0.1780 - val_accuracy: 0.9499\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0989 - accuracy: 0.9791 - val_loss: 0.0732 - val_accuracy: 0.9870\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2587 - accuracy: 0.9304 - val_loss: 0.5593 - val_accuracy: 0.8306\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1971 - accuracy: 0.9553 - val_loss: 0.0905 - val_accuracy: 0.9875\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0587 - accuracy: 0.9946 - val_loss: 0.0433 - val_accuracy: 0.9967\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0306 - accuracy: 0.9984 - val_loss: 0.0253 - val_accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0187 - accuracy: 0.9994 - val_loss: 0.0163 - val_accuracy: 0.9995\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0123 - accuracy: 0.9998 - val_loss: 0.0114 - val_accuracy: 0.9998\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0086 - accuracy: 0.9999 - val_loss: 0.0083 - val_accuracy: 0.9999\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0062 - accuracy: 0.9999 - val_loss: 0.0062 - val_accuracy: 0.9999\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9999\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
    "                           output_dim=embedding_size,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great, we reach 100% validation accuracy! Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
    "            for sequence in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to convert some dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-02\n",
      "1789-09-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let's write a little helper function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = model.predict_classes(X)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Granted, there are certainly much easier ways to write a date conversion tool (e.g., using regular expressions or even basic string manipulation), but you have to admit that using neural networks is way cooler. ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let's build a more powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second version: feeding the shifted targets to the decoder (teacher forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.\n",
    "\n",
    "Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n",
    "\n",
    "During inference, we won't know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n",
    "\n",
    "But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass it it the vector output by the encoder? Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n",
    "\n",
    "Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the decoder's training inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]], dtype=int32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the model. It's not a simple sequential model anymore, so let's use the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6901 - accuracy: 0.3714 - val_loss: 1.4141 - val_accuracy: 0.4604\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.2042 - accuracy: 0.5562 - val_loss: 0.8934 - val_accuracy: 0.6841\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6149 - accuracy: 0.7875 - val_loss: 0.3466 - val_accuracy: 0.8929\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1896 - accuracy: 0.9580 - val_loss: 0.0965 - val_accuracy: 0.9876\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0862 - accuracy: 0.9874 - val_loss: 0.0402 - val_accuracy: 0.9985\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0275 - accuracy: 0.9995 - val_loss: 0.0228 - val_accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0481 - accuracy: 0.9916 - val_loss: 0.0407 - val_accuracy: 0.9964\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0149 - accuracy: 0.9998 - val_loss: 0.0098 - val_accuracy: 0.9999\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1,\n",
    "    output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
    "    lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2,\n",
    "    output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
    "    decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
    "                                    activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input],\n",
    "                           outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also reaches 100% validation accuracy, but it does so even faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's once again use the model to make some predictions. This time we need to predict characters one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 53 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5809305950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 53 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5809305950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works fine! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third version: using TF-Addons's seq2seq implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build exactly the same model, but using TF-Addon's seq2seq API. The implementation below is almost very similar to the TFA example higher in this notebook, except without the model input to specify the output sequence length, for simplicity (but you can easily add it back in if you need it for your projects, when the output sequences have very different lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.6782 - accuracy: 0.3686 - val_loss: 1.4574 - val_accuracy: 0.4304\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 1.3472 - accuracy: 0.4792 - val_loss: 1.2293 - val_accuracy: 0.5292\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.8811 - accuracy: 0.6861 - val_loss: 0.5698 - val_accuracy: 0.8045\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.3547 - accuracy: 0.8915 - val_loss: 0.1697 - val_accuracy: 0.9689\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1385 - accuracy: 0.9768 - val_loss: 0.0644 - val_accuracy: 0.9953\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0423 - accuracy: 0.9983 - val_loss: 0.0331 - val_accuracy: 0.9988\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0206 - accuracy: 0.9998 - val_loss: 0.0171 - val_accuracy: 0.9998\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0549 - accuracy: 0.9898 - val_loss: 0.0185 - val_accuracy: 0.9998\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0112 - accuracy: 0.9999 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9999\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once again, 100% validation accuracy! To use the model, we can just reuse the `predict_date_strs()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there's a much more efficient way to perform inference. Until now, during inference, we've run the model once for each new character. Instead, we can create a new decoder, based on the previously trained layers, but using a `GreedyEmbeddingSampler` instead of a `TrainingSampler`.\n",
    "\n",
    "At each time step, the `GreedyEmbeddingSampler` will compute the argmax of the decoder's outputs, and run the resulting token IDs through the decoder's embedding layer. Then it will feed the resulting embeddings to the decoder's LSTM cell at the next time step. This way, we only need to run the decoder once to get the full prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "    embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=max_output_length)\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
    "                                     outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes:\n",
    "* The `GreedyEmbeddingSampler` needs the `start_tokens` (a vector containing the start-of-sequence ID for each decoder sequence), and the `end_token` (the decoder will stop decoding a sequence once the model outputs this token).\n",
    "* We must set `maximum_iterations` when creating the `BasicDecoder`, or else it may run into an infinite loop (if the model never outputs the end token for at least one of the sequences). This would force you would to restart the Jupyter kernel.\n",
    "* The decoder inputs are not needed anymore, since all the decoder inputs are generated dynamically based on the outputs from the previous time step.\n",
    "* The model's outputs are `final_outputs.sample_id` instead of the softmax of `final_outputs.rnn_outputs`. This allows us to directly get the argmax of the model's outputs. If you prefer to have access to the logits, you can replace `final_outputs.sample_id` with `final_outputs.rnn_outputs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a simple function that uses the model to perform the date format conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that it really is faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607 ms ± 15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.2 ms ± 1.39 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more than a 10x speedup! And it would be even more if we were handling longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth version: using TF-Addons's seq2seq implementation with a scheduled sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: due to a TF bug, this version only works using TensorFlow 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we trained the previous model, at each time step _t_ we gave the model the target token for time step _t_ - 1. However, at inference time, the model did not get the previous target at each time step. Instead, it got the previous prediction. So there is a discrepancy between training and inference, which may lead to disappointing performance. To alleviate this, we can gradually replace the targets with the predictions, during training. For this, we just need to replace the `TrainingSampler` with a `ScheduledEmbeddingTrainingSampler`, and use a Keras callback to gradually increase the `sampling_probability` (i.e., the probability that the decoder will use the prediction from the previous time step rather than the target for the previous time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 1.6782 - accuracy: 0.3686 - val_loss: 1.4575 - val_accuracy: 0.4304\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 1.3857 - accuracy: 0.4590 - val_loss: 1.2498 - val_accuracy: 0.5246\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 1.0742 - accuracy: 0.5999 - val_loss: 0.8688 - val_accuracy: 0.6754\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.6618 - accuracy: 0.7601 - val_loss: 0.4599 - val_accuracy: 0.8322\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.3445 - accuracy: 0.8854 - val_loss: 0.2544 - val_accuracy: 0.9220\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.2643 - accuracy: 0.9213 - val_loss: 0.1739 - val_accuracy: 0.9510\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.1338 - accuracy: 0.9649 - val_loss: 0.1080 - val_accuracy: 0.9724\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0941 - accuracy: 0.9783 - val_loss: 0.0635 - val_accuracy: 0.9870\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0475 - accuracy: 0.9909 - val_loss: 0.0374 - val_accuracy: 0.9936\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0354 - accuracy: 0.9937 - val_loss: 0.0240 - val_accuracy: 0.9966\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0188 - accuracy: 0.9974 - val_loss: 0.0164 - val_accuracy: 0.9976\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0130 - accuracy: 0.9984 - val_loss: 0.0108 - val_accuracy: 0.9991\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.1565 - accuracy: 0.9665 - val_loss: 0.5308 - val_accuracy: 0.8305\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0839 - accuracy: 0.9841 - val_loss: 0.0239 - val_accuracy: 0.9977\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0163 - accuracy: 0.9985 - val_loss: 0.0120 - val_accuracy: 0.9988\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0090 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9992\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0062 - accuracy: 0.9995 - val_loss: 0.0053 - val_accuracy: 0.9997\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0043 - accuracy: 0.9997 - val_loss: 0.0046 - val_accuracy: 0.9994\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 0.0032 - val_accuracy: 0.9998\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.0028 - val_accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_epochs = 20\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(\n",
    "    sampling_probability=0.,\n",
    "    embedding_fn=decoder_embedding_layer)\n",
    "# we must set the sampling_probability after creating the sampler\n",
    "# (see https://github.com/tensorflow/addons/pull/1714)\n",
    "sampler.sampling_probability = tf.Variable(0.)\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "def update_sampling_probability(epoch, logs):\n",
    "    proba = min(1.0, epoch / (n_epochs - 10))\n",
    "    sampler.sampling_probability.assign(proba)\n",
    "\n",
    "sampling_probability_cb = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=update_sampling_probability)\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=n_epochs,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid),\n",
    "                    callbacks=[sampling_probability_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite 100% validation accuracy, but close enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference, we could do the exact same thing as earlier, using a `GreedyEmbeddingSampler`. However, just for the sake of completeness, let's use a `SampleEmbeddingSampler` instead. It's almost the same thing, except that instead of using the argmax of the model's output to find the token ID, it treats the outputs as logits and uses them to sample a token ID randomly. This can be useful when you want to generate text. The `softmax_temperature` argument serves the \n",
    "same purpose as when we generated Shakespeare-like text (the higher this argument, the more random the generated text will be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_temperature = tf.Variable(1.)\n",
    "\n",
    "inference_sampler = tfa.seq2seq.sampler.SampleEmbeddingSampler(\n",
    "    embedding_fn=decoder_embedding_layer,\n",
    "    softmax_temperature=softmax_temperature)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=max_output_length)\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
    "                                     outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creative_predict_date_strs(date_strs, temperature=1.0):\n",
    "    softmax_temperature.assign(temperature)\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2000-05-01']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates look good at room temperature. Now let's heat things up a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7179307-29', '200040?400']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"],\n",
    "                           temperature=5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, the dates are overcooked, now. Let's call them \"creative\" dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth version: using TFA seq2seq, the Keras subclassing API and attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences in this problem are pretty short, but if we wanted to tackle longer sequences, we would probably have to use attention mechanisms. While it's possible to code our own implementation, it's simpler and more efficient to use TF-Addons's implementation instead. Let's do that now, this time using Keras' subclassing API.\n",
    "\n",
    "**Warning**: due to a TensorFlow bug (see [this issue](https://github.com/tensorflow/addons/issues/1153) for details), the `get_initial_state()` method fails in eager mode, so for now we have to use the subclassing API, as Keras automatically calls `tf.function()` on the `call()` method (so it runs in graph mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we've reverted back to using the `TrainingSampler`, for simplicity (but you can easily tweak it to use a `ScheduledEmbeddingTrainingSampler` instead). We also use a `GreedyEmbeddingSampler` during inference, so this class is pretty easy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTranslation(keras.models.Model):\n",
    "    def __init__(self, units=128, encoder_embedding_size=32,\n",
    "                 decoder_embedding_size=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_embedding = keras.layers.Embedding(\n",
    "            input_dim=len(INPUT_CHARS) + 1,\n",
    "            output_dim=encoder_embedding_size)\n",
    "        self.encoder = keras.layers.LSTM(units,\n",
    "                                         return_sequences=True,\n",
    "                                         return_state=True)\n",
    "        self.decoder_embedding = keras.layers.Embedding(\n",
    "            input_dim=len(OUTPUT_CHARS) + 2,\n",
    "            output_dim=decoder_embedding_size)\n",
    "        self.attention = tfa.seq2seq.LuongAttention(units)\n",
    "        decoder_inner_cell = keras.layers.LSTMCell(units)\n",
    "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n",
    "            cell=decoder_inner_cell,\n",
    "            attention_mechanism=self.attention)\n",
    "        output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.TrainingSampler(),\n",
    "            output_layer=output_layer)\n",
    "        self.inference_decoder = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "                embedding_fn=self.decoder_embedding),\n",
    "            output_layer=output_layer,\n",
    "            maximum_iterations=max_output_length)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        encoder_input, decoder_input = inputs\n",
    "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = self.encoder(\n",
    "            encoder_embeddings,\n",
    "            training=training)\n",
    "        encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        self.attention(encoder_outputs,\n",
    "                       setup_memory=True)\n",
    "        \n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
    "\n",
    "        decoder_initial_state = self.decoder_cell.get_initial_state(\n",
    "            decoder_embeddings)\n",
    "        decoder_initial_state = decoder_initial_state.clone(\n",
    "            cell_state=encoder_state)\n",
    "        \n",
    "        if training:\n",
    "            decoder_outputs, _, _ = self.decoder(\n",
    "                decoder_embeddings,\n",
    "                initial_state=decoder_initial_state,\n",
    "                training=training)\n",
    "        else:\n",
    "            start_tokens = tf.zeros_like(encoder_input[:, 0]) + sos_id\n",
    "            decoder_outputs, _, _ = self.inference_decoder(\n",
    "                decoder_embeddings,\n",
    "                initial_state=decoder_initial_state,\n",
    "                start_tokens=start_tokens,\n",
    "                end_token=0)\n",
    "\n",
    "        return tf.nn.softmax(decoder_outputs.rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 2.1391 - accuracy: 0.2346 - val_loss: 1.9953 - val_accuracy: 0.2805\n",
      "Epoch 2/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 2.0472 - accuracy: 0.2751 - val_loss: 2.7959 - val_accuracy: 0.1375\n",
      "Epoch 3/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 1.4588 - accuracy: 0.4721 - val_loss: 1.2255 - val_accuracy: 0.5621\n",
      "Epoch 4/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 1.0513 - accuracy: 0.6208 - val_loss: 0.7987 - val_accuracy: 0.7031\n",
      "Epoch 5/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.4682 - accuracy: 0.8495 - val_loss: 0.2970 - val_accuracy: 0.9006\n",
      "Epoch 6/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.2616 - accuracy: 0.9138 - val_loss: 0.1722 - val_accuracy: 0.9423\n",
      "Epoch 7/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0627 - accuracy: 0.9852 - val_loss: 0.0324 - val_accuracy: 0.9952\n",
      "Epoch 8/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.1023 - accuracy: 0.9801 - val_loss: 0.0399 - val_accuracy: 0.9962\n",
      "Epoch 9/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0182 - accuracy: 0.9993 - val_loss: 0.0111 - val_accuracy: 0.9999\n",
      "Epoch 10/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0093 - accuracy: 0.9995 - val_loss: 0.0117 - val_accuracy: 0.9991\n",
      "Epoch 11/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 0.0063 - val_accuracy: 0.9997\n",
      "Epoch 12/25\n",
      "313/313 [==============================] - 14s 43ms/step - loss: 0.0151 - accuracy: 0.9972 - val_loss: 0.0101 - val_accuracy: 0.9992\n",
      "Epoch 13/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 0.9994\n",
      "Epoch 14/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 0.9994\n",
      "Epoch 15/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 0.9994\n",
      "Epoch 16/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9997\n",
      "Epoch 17/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 8.7506e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9997\n",
      "Epoch 18/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 7.0376e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9997\n",
      "Epoch 19/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 5.7279e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9997\n",
      "Epoch 20/25\n",
      "313/313 [==============================] - 14s 43ms/step - loss: 4.6947e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9997\n",
      "Epoch 21/25\n",
      "313/313 [==============================] - 14s 43ms/step - loss: 3.8742e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9997\n",
      "Epoch 22/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 3.2071e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9997\n",
      "Epoch 23/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 2.6652e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996\n",
      "Epoch 24/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 2.2199e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9996\n",
      "Epoch 25/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 1.8539e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = DateTranslation()\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=25,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite 100% validation accuracy, but close. It took a bit longer to converge this time, but there were also more parameters and more computations per iteration. And we did not use a scheduled sampler.\n",
    "\n",
    "To use the model, we can write yet another little function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs_v2(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    X_decoder = tf.zeros(shape=(len(X), max_output_length), dtype=tf.int32)\n",
    "    Y_probas = model.predict([X, X_decoder])\n",
    "    Y_pred = tf.argmax(Y_probas, axis=-1)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs_v2([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still a few interesting features from TF-Addons that you may want to look at:\n",
    "* Using a `BeamSearchDecoder` rather than a `BasicDecoder` for inference. Instead of outputing the character with the highest probability, this decoder keeps track of the several candidates, and keeps only the most likely sequences of candidates (see chapter 16 in the book for more details).\n",
    "* Setting masks or specifying `sequence_length` if the input or target sequences may have very different lengths.\n",
    "* Using a `ScheduledOutputTrainingSampler`, which gives you more flexibility than the `ScheduledEmbeddingTrainingSampler` to decide how to feed the output at time _t_ to the cell at time _t_+1. By default it feeds the outputs directly to cell, without computing the argmax ID and passing it through an embedding layer. Alternatively, you specify a `next_inputs_fn` function that will be used to convert the cell outputs to inputs at the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "_Exercise: Go through TensorFlow's [Neural Machine Translation with Attention tutorial](https://homl.info/nmttuto)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply open the Colab and follow its instructions. Alternatively, if you want a simpler example of using TF-Addons's seq2seq implementation for Neural Machine Translation (NMT), look at the solution to the previous question. The last model implementation will give you a simpler example of using TF-Addons to build an NMT model using attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "_Exercise: Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의**: 이 연습문제 코드를 실행하려면 허깅페이스(huggingface)의 트랜스포머 라이브러리를 설치해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use recent language models is to use the excellent [transformers library](https://huggingface.co/transformers/), open sourced by Hugging Face. It provides many modern neural net architectures (including BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet and more) for Natural Language Processing (NLP), including many pretrained models. It relies on either TensorFlow or PyTorch. Best of all: it's amazingly simple to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load a pretrained model. In this example, we will use OpenAI's GPT model, with an additional Language Model on top (just a linear layer with weights tied to the input embeddings). Let's import it and load the pretrained weights (this will download about 445MB of data to `~/.cache/torch/transformers`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc570b5d25242989adb8115208ed09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=656.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2005518a7ee74c91a589eb5e0c5bfb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466312920.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_tf_utils:All model checkpoint weights were used when initializing TFOpenAIGPTLMHeadModel.\n",
      "\n",
      "WARNING:transformers.modeling_tf_utils:All the weights of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need a specialized tokenizer for this model. This one will try to use the [spaCy](https://spacy.io/) and [ftfy](https://pypi.org/project/ftfy/) libraries if they are installed, or else it will fall back to BERT's `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most use cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ddfdc4e8c04e6aaadc25912c81b316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=815973.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edb771b992541f0a35611f9725539a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=458495.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the tokenizer to tokenize and encode the prompt text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]], dtype=int32)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy! Next, let's use the model to generate text after the prompt. We will generate 5 different sentences, each starting with the prompt text, followed by 40 additional tokens. For an explanation of what all the hyperparameters do, make sure to check out this great [blog post](https://huggingface.co/blog/how-to-generate) by Patrick von Platen (from Hugging Face). You can play around with the hyperparameters to try to obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   509,   481,  9313,  6640,   498,  1389, 11031,\n",
       "          239,   481,  2204,   544,   525,   481,  4906,  3659,   498,\n",
       "          481,   653,   911,   498,     8, 38648,   641,  1236,   481,\n",
       "         5018,   498,   481, 21368,   488,   481,  6404,   948, 35174,\n",
       "          715,  1076,   763,   641,  2520],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   980,  1981,   557,   481,  2827,   498,   481,  1028,\n",
       "          498,  9606,   239,   244, 40477,   244,   862,  1256,   240,\n",
       "          547,  2185,   239,   244,  7395, 21800,   513,  2185,   239,\n",
       "          244,   599,   636,   512,   649,   485,   788,   257,   244,\n",
       "        40477,   481,  2228,   535,   741],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239,   481,  3549,   498,   589,   547, 16375,   240,\n",
       "         4258,  1076,   498,   547,  5080,   260,  2228,   240,  1485,\n",
       "          500,   481,  6623,  2219,   481, 28230,   239,   481, 10445,\n",
       "          535,  1835,   240,   481,  1424,  8815,   260,   618,   240,\n",
       "          980,  1233,   524,  5424,   677],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   487,   509,  3105,   500,   240,   616,   908,   487,\n",
       "          558,  2160,   781,   575,   240,   507,   544,  6322,   500,\n",
       "          481,  1279,   498,  2857, 24711,   504,   481,  7361,  2075,\n",
       "          498,   481, 16187,   240,   524,  1584,   759,   580,  1132,\n",
       "          822,   481,  4644,   498,  2857],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240, 40477,   488,   481,  1922,  4198,   535,  8908,\n",
       "          240,   834,   240,   481,  2185,   498,  1092,   239, 40477,\n",
       "          244,   500,   481,  1385,   498,   481,  2878, 13658,   240,\n",
       "         2122,  2821,  1085,   589,  8162,   240,   244, 40477,   655,\n",
       "          544,   597,  1203,  3126,   500]], dtype=int32)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's decode the generated sequences and print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle, was the largest collection of such affairs. the problem is that the descendents of the earls of astoria were under the rule of the sages and the throne took precedence over those who were forced\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle has passed as the beginning of the age of kings. \" \n",
      " \" well done, my lord. \" velvet complimented her lord. \" what would you like to see? \" \n",
      " the lady's eyes\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle. the bones of all my comrades, including those of my ex - lady, lie in the hollow beneath the eaves. the crow's eye, the great griffin - king, has set his battlea\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle he was born in, this door he had placed before him, it is located in the heart of galdir on the outer edge of the isle, his line can be found through the houses of gal\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, \n",
      " and the pendragon's portal, too, the lord of light. \n",
      " \" in the course of the seven pillars, ye shall find all treasure, \" \n",
      " there is now three bodies in\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try more recent (and larger) models, such as GPT-2, CTRL, Transformer-XL or XLNet, which are all available as pretrained models in the transformers library, including variants with Language Models on top. The preprocessing steps vary slightly between models, so make sure to check out this [generation example](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py) from the transformers documentation (this example uses PyTorch, but it will work with very little tweaks, such as adding `TF` at the beginning of the model class name, removing the `.to()` method calls, and using `return_tensors=\"tf\"` instead of `\"pt\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoyed this chapter! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.2 on Python 3.6 (CUDA 10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "021e44d3eef34e87ac9526b037e339dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0edb771b992541f0a35611f9725539a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_632ee4d40b2e48b99b279900022572ca",
        "IPY_MODEL_25cbbb5997d14f20b64c239e22599aba"
       ],
       "layout": "IPY_MODEL_d6fab047cad541c7a2415452e077858b"
      }
     },
     "1a17298be1aa444c853f6a6f698a9806": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1ad7b72ff7f04822b240bcd37b13decf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f109e361ef5433d8ff0879eb77aead6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4c28cee8e5604caa9e7e7ba25c6c69a9",
       "placeholder": "​",
       "style": "IPY_MODEL_6e75a1d5f67a44bf806793a33c765acb",
       "value": " 656/656 [00:39&lt;00:00, 16.6B/s]"
      }
     },
     "2005518a7ee74c91a589eb5e0c5bfb01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c8ee39eb7c96498e8c26cb0164c2da60",
        "IPY_MODEL_ea058ffa780a4c4d9d077c8a7e6a68bc"
       ],
       "layout": "IPY_MODEL_9d958cd57a354c03ba558f4177aa937d"
      }
     },
     "25cbbb5997d14f20b64c239e22599aba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6a1a859ee18a4567877ca3b41b594f8c",
       "placeholder": "​",
       "style": "IPY_MODEL_97467a476670412097e51a838241a72a",
       "value": " 458k/458k [00:00&lt;00:00, 555kB/s]"
      }
     },
     "356ffaeb18cf476d8f38ff19609bbeb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f3f6133303144e0a47c63fcc69205bb",
       "max": 815973.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d629495d1d4b420eb23929bca1268fca",
       "value": 815973.0
      }
     },
     "48d21febab09411ca8a534973d040c5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4a673857eaf042e3a919b8d4ed7fab5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b211d9350cd84d8e8b6a5f393217ed5c",
       "max": 656.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1a17298be1aa444c853f6a6f698a9806",
       "value": 656.0
      }
     },
     "4c28cee8e5604caa9e7e7ba25c6c69a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4dc570b5d25242989adb8115208ed09a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4a673857eaf042e3a919b8d4ed7fab5e",
        "IPY_MODEL_1f109e361ef5433d8ff0879eb77aead6"
       ],
       "layout": "IPY_MODEL_1ad7b72ff7f04822b240bcd37b13decf"
      }
     },
     "5f3f6133303144e0a47c63fcc69205bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "632ee4d40b2e48b99b279900022572ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9496580233e146ea9003820622982180",
       "max": 458495.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_48d21febab09411ca8a534973d040c5f",
       "value": 458495.0
      }
     },
     "6a1a859ee18a4567877ca3b41b594f8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e75a1d5f67a44bf806793a33c765acb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "810618d342034ab2b88aadaaf446e53d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "90e896893b7840b99e2c12d301298792": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b50e93b04aa0463380ef4e29372c0f47",
       "placeholder": "​",
       "style": "IPY_MODEL_e70e518f45994b2bbf423c9ba861155e",
       "value": " 816k/816k [00:02&lt;00:00, 309kB/s]"
      }
     },
     "9496580233e146ea9003820622982180": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96ddfdc4e8c04e6aaadc25912c81b316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_356ffaeb18cf476d8f38ff19609bbeb5",
        "IPY_MODEL_90e896893b7840b99e2c12d301298792"
       ],
       "layout": "IPY_MODEL_c1b0ec4c17ac4e939b4f8c32d16e8cc0"
      }
     },
     "97467a476670412097e51a838241a72a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9d958cd57a354c03ba558f4177aa937d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a77bde688d5d4a0f8c32509ba5eb3c69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b211d9350cd84d8e8b6a5f393217ed5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b50e93b04aa0463380ef4e29372c0f47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1b0ec4c17ac4e939b4f8c32d16e8cc0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8ee39eb7c96498e8c26cb0164c2da60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a77bde688d5d4a0f8c32509ba5eb3c69",
       "max": 466312920.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_810618d342034ab2b88aadaaf446e53d",
       "value": 466312920.0
      }
     },
     "d629495d1d4b420eb23929bca1268fca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "d6fab047cad541c7a2415452e077858b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e70e518f45994b2bbf423c9ba861155e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea058ffa780a4c4d9d077c8a7e6a68bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ecc05705cb29442bb8ee07e31acd75d5",
       "placeholder": "​",
       "style": "IPY_MODEL_021e44d3eef34e87ac9526b037e339dd",
       "value": " 466M/466M [00:38&lt;00:00, 12.0MB/s]"
      }
     },
     "ecc05705cb29442bb8ee07e31acd75d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
